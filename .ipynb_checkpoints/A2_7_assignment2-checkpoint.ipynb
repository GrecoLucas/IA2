{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92694cbd-93e3-4d6f-9ce7-7034fc8bdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce76b480-ea10-4d21-86c9-a19188bdb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(type, df, test_size, neighbors, n_estimators=100, c_value=1.0, random_state=42):\n",
    "    # Features: relevant columns of the data\n",
    "    feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                    'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                    'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                    'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                    'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                    'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                    'fighter2_Age']\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = pd.get_dummies(df, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "\n",
    "    df['target'] = df['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "    df = df.dropna(subset=['target'])\n",
    "    \n",
    "    stance_cols = [col for col in df.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "\n",
    "    X = df[feature_cols + stance_cols]\n",
    "    y = df['target']\n",
    "    \n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if(type == \"decisionTree\"):\n",
    "        model = DecisionTreeClassifier(random_state=random_state)\n",
    "    elif(type == \"K-nearestNeighbors\"):\n",
    "        model = KNeighborsClassifier(neighbors)\n",
    "    elif(type == \"randomForest\"):\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    elif(type == \"svm\"):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        model = SVC(C=c_value, kernel='rbf', random_state=random_state, probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model, X_test, y_test, scaler\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, X_test, y_test\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    t = end - start\n",
    "    return accuracy, precision, recall, f1, t, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363946e7-a128-4987-bea6-cb409ae7ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred, model_name, output_dir):\n",
    "    \"\"\"Plot confusion matrix for a model.\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fighter 2 Wins', 'Fighter 1 Wins'],\n",
    "                yticklabels=['Fighter 2 Wins', 'Fighter 1 Wins'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(model, X_test, y_test, model_name, output_dir):\n",
    "    \"\"\"Plot ROC curve for a model.\"\"\"\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_proba = model.decision_function(X_test)\n",
    "        else:\n",
    "            print(f\"Cannot generate ROC curve for {model_name}: model doesn't support probability prediction\")\n",
    "            return\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'roc_curve_{model_name.lower().replace(\" \", \"_\")}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        return roc_auc\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating ROC curve for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, title, output_dir, cv=5):\n",
    "    \"\"\"Plot learning curve for a model.\"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        random_state=42)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title(f'Learning Curve - {title}')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'learning_curve_{title.lower().replace(\" \", \"_\")}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_with_cv(model, X, y, cv=5):\n",
    "    \"\"\"Evaluate model with cross-validation.\"\"\"\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    return cv_scores.mean(), cv_scores.std()\n",
    "\n",
    "def make_graph_decision_tree(measure, xAxis, yAxis, output_dir):\n",
    "    \"\"\"Create a graph showing the effect of test size proportion on model performance.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(xAxis, yAxis, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Effects of training data size in {measure}')\n",
    "    plt.xlabel('Percentage of Data used for Testing')\n",
    "    plt.ylabel(f'{measure}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(xAxis)\n",
    "\n",
    "    plt.ylim(max(min(yAxis)-0.05, 0), max(yAxis) + 0.05)  \n",
    "    plt.savefig(os.path.join(output_dir, f'{measure}_decision_trees.png'))\n",
    "    plt.close()\n",
    "\n",
    "def make_graph_k_neighbors(measure, xAxis, yAxis, output_dir):\n",
    "    \"\"\"Create a graph showing the effect of number of neighbors on model performance.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(xAxis, yAxis, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Effects of number of neighbors in {measure}')\n",
    "    plt.xlabel('Number of Neighbors')\n",
    "    plt.ylabel(f'{measure}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(xAxis)\n",
    "\n",
    "    plt.ylim(max(min(yAxis)-0.05, 0), max(yAxis) + 0.05)  \n",
    "    plt.savefig(os.path.join(output_dir, f'{measure}_number_neighbors.png'))\n",
    "    plt.close()\n",
    "\n",
    "def make_graph_models(measure, models, yAxis, output_dir):\n",
    "    \"\"\"Create a graph comparing different models.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.bar(models, yAxis, color='b')\n",
    "    plt.title(f'Comparison between models: {measure}')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(f'{measure}')\n",
    "    plt.grid(True, axis='y') \n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.ylim(max(min(yAxis)-0.05, 0), max(yAxis) + 0.05)  \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{measure}_models.png'))\n",
    "    plt.close()\n",
    "\n",
    "def make_graph_random_forest(measure, xAxis, yAxis, output_dir):\n",
    "    \"\"\"Create a graph showing the effect of number of estimators on RF performance.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(xAxis, yAxis, marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Effects of number of estimators in {measure}')\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel(f'{measure}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(xAxis)\n",
    "\n",
    "    plt.ylim(max(min(yAxis)-0.05, 0), max(yAxis) + 0.05)  \n",
    "    plt.savefig(os.path.join(output_dir, f'{measure}_random_forest.png'))\n",
    "    plt.close()\n",
    "\n",
    "def make_graph_svm(measure, xAxis, yAxis, output_dir):\n",
    "    \"\"\"Create a graph showing the effect of C parameter on SVM performance.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(xAxis, yAxis, marker='o', linestyle='-', color='r')\n",
    "    plt.title(f'Effects of C parameter in {measure} (SVM)')\n",
    "    plt.xlabel('C Parameter Value')\n",
    "    plt.ylabel(f'{measure}')\n",
    "    plt.xscale('log') \n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.ylim(max(min(yAxis)-0.05, 0), max(yAxis) + 0.05)  \n",
    "    plt.savefig(os.path.join(output_dir, f'{measure}_svm.png'))\n",
    "    plt.close()\n",
    "\n",
    "def compare_decision_tree(df, output_dir):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    cv_scores = []\n",
    "    \n",
    "    for i in range(9, 0, -1):\n",
    "        start = time.time()\n",
    "        model, X_test, y_test = create_model(\"decisionTree\", df, i/10, 0)\n",
    "        end = time.time()\n",
    "        train_time += [end-start]\n",
    "        acc, prec, rec, f1_, time_, y_pred = test_model(model, X_test, y_test)\n",
    "        accuracy += [acc]\n",
    "        precision += [prec]\n",
    "        recall += [rec]\n",
    "        f1 += [f1_]\n",
    "        test_time += [time_]\n",
    "        \n",
    "        # Cross-validation\n",
    "        feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                        'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                        'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                        'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                        'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                        'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                        'fighter2_Age']\n",
    "        \n",
    "        df_temp = df.copy()\n",
    "        df_temp = pd.get_dummies(df_temp, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "        df_temp['target'] = df_temp['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "        df_temp = df_temp.dropna(subset=['target'])\n",
    "        \n",
    "        stance_cols = [col for col in df_temp.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "        X_full = df_temp[feature_cols + stance_cols].fillna(df_temp[feature_cols + stance_cols].median())\n",
    "        y_full = df_temp['target']\n",
    "        \n",
    "        cv_mean, cv_std = evaluate_with_cv(DecisionTreeClassifier(random_state=42), X_full, y_full)\n",
    "        cv_scores.append(cv_mean)\n",
    "        \n",
    "        # Generate confusion matrix for the best test size (smallest one)\n",
    "        if i == 1:\n",
    "            plot_confusion_matrix(y_test, y_pred, \"Decision Tree\", output_dir)\n",
    "            plot_roc_curve(model, X_test, y_test, \"Decision Tree\", output_dir)\n",
    "            plot_learning_curve(DecisionTreeClassifier(random_state=42), X_full, y_full, \"Decision Tree\", output_dir)\n",
    "    \n",
    "    train_sizes = [x/10 for x in range(9,0,-1)]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n----- DECISION TREE RESULTS BY TEST SIZE -----\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test Size': train_sizes,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Score': cv_scores,\n",
    "        'Training Time': train_time,\n",
    "        'Testing Time': test_time\n",
    "    })\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    make_graph_decision_tree(\"Accuracy\", train_sizes, accuracy, output_dir)\n",
    "    make_graph_decision_tree(\"Precision\", train_sizes, precision, output_dir)\n",
    "    make_graph_decision_tree(\"Recall\", train_sizes, recall, output_dir)\n",
    "    make_graph_decision_tree(\"F1 Score\", train_sizes, f1, output_dir)\n",
    "    make_graph_decision_tree(\"CV Score\", train_sizes, cv_scores, output_dir)\n",
    "    make_graph_decision_tree(\"Training Time\", train_sizes, train_time, output_dir)\n",
    "    make_graph_decision_tree(\"Testing Time\", train_sizes, test_time, output_dir)\n",
    "\n",
    "def compare_k_nearest_neighbors(df, output_dir):\n",
    "    \"\"\"Compare performance of K-Nearest Neighbors models with different numbers of neighbors.\"\"\"\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Prepare data once for cross-validation\n",
    "    feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                    'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                    'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                    'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                    'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                    'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                    'fighter2_Age']\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp = pd.get_dummies(df_temp, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "    df_temp['target'] = df_temp['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "    df_temp = df_temp.dropna(subset=['target'])\n",
    "    \n",
    "    stance_cols = [col for col in df_temp.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "    X_full = df_temp[feature_cols + stance_cols].fillna(df_temp[feature_cols + stance_cols].median())\n",
    "    y_full = df_temp['target']\n",
    "    \n",
    "    best_k = 0\n",
    "    best_accuracy = 0\n",
    "    best_model_data = None\n",
    "    \n",
    "    for i in range(1, 31, 1):\n",
    "        start = time.time()\n",
    "        model, X_test, y_test = create_model(\"K-nearestNeighbors\", df, 0.1, i)\n",
    "        end = time.time()\n",
    "        train_time += [end-start]\n",
    "        acc, prec, rec, f1_, time_, y_pred = test_model(model, X_test, y_test)\n",
    "        accuracy += [acc]\n",
    "        precision += [prec]\n",
    "        recall += [rec]\n",
    "        f1 += [f1_]\n",
    "        test_time += [time_]\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_mean, cv_std = evaluate_with_cv(KNeighborsClassifier(i), X_full, y_full)\n",
    "        cv_scores.append(cv_mean)\n",
    "        \n",
    "        # Track best model\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_k = i\n",
    "            best_model_data = (model, X_test, y_test, y_pred)\n",
    "    \n",
    "    num_neigs = [x for x in range(1,31,1)]\n",
    "    \n",
    "    # Generate confusion matrix and ROC curve for best k\n",
    "    if best_model_data:\n",
    "        model, X_test, y_test, y_pred = best_model_data\n",
    "        plot_confusion_matrix(y_test, y_pred, f\"KNN (k={best_k})\", output_dir)\n",
    "        plot_roc_curve(model, X_test, y_test, f\"KNN (k={best_k})\", output_dir)\n",
    "        plot_learning_curve(KNeighborsClassifier(best_k), X_full, y_full, f\"KNN (k={best_k})\", output_dir)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n----- KNN RESULTS BY NUMBER OF NEIGHBORS -----\")\n",
    "    k_samples = [1, 5, 10, 15, 20, 25, 30]\n",
    "    k_indices = [k-1 for k in k_samples]\n",
    "    results_df = pd.DataFrame({\n",
    "        'K Neighbors': [num_neigs[i] for i in k_indices],\n",
    "        'Accuracy': [accuracy[i] for i in k_indices],\n",
    "        'Precision': [precision[i] for i in k_indices],\n",
    "        'Recall': [recall[i] for i in k_indices],\n",
    "        'F1 Score': [f1[i] for i in k_indices],\n",
    "        'CV Score': [cv_scores[i] for i in k_indices],\n",
    "        'Training Time': [train_time[i] for i in k_indices],\n",
    "        'Testing Time': [test_time[i] for i in k_indices]\n",
    "    })\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    best_k_idx = np.argmax(accuracy)\n",
    "    print(f\"\\nBest K value found: {best_k_idx + 1}\")\n",
    "    print(f\"Maximum accuracy: {accuracy[best_k_idx]:.4f}\")\n",
    "    print(f\"Precision: {precision[best_k_idx]:.4f}\")\n",
    "    print(f\"Recall: {recall[best_k_idx]:.4f}\")\n",
    "    print(f\"F1 Score: {f1[best_k_idx]:.4f}\")\n",
    "    \n",
    "    make_graph_k_neighbors(\"Accuracy\", num_neigs, accuracy, output_dir)\n",
    "    make_graph_k_neighbors(\"Precision\", num_neigs, precision, output_dir)\n",
    "    make_graph_k_neighbors(\"Recall\", num_neigs, recall, output_dir)\n",
    "    make_graph_k_neighbors(\"F1 Score\", num_neigs, f1, output_dir)\n",
    "    make_graph_k_neighbors(\"CV Score\", num_neigs, cv_scores, output_dir)\n",
    "    make_graph_k_neighbors(\"Training Time\", num_neigs, train_time, output_dir)\n",
    "    make_graph_k_neighbors(\"Testing Time\", num_neigs, test_time, output_dir)\n",
    "\n",
    "def compare_random_forest(df, output_dir):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    cv_scores = []\n",
    "    n_estimators_values = [10, 50, 100, 150, 200]\n",
    "    \n",
    "    # Prepare data for cross-validation\n",
    "    feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                    'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                    'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                    'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                    'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                    'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                    'fighter2_Age']\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp = pd.get_dummies(df_temp, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "    df_temp['target'] = df_temp['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "    df_temp = df_temp.dropna(subset=['target'])\n",
    "    \n",
    "    stance_cols = [col for col in df_temp.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "    X_full = df_temp[feature_cols + stance_cols].fillna(df_temp[feature_cols + stance_cols].median())\n",
    "    y_full = df_temp['target']\n",
    "    \n",
    "    best_n = 0\n",
    "    best_accuracy = 0\n",
    "    best_model_data = None\n",
    "    \n",
    "    for n_estimators in n_estimators_values:\n",
    "        start = time.time()\n",
    "        model, X_test, y_test = create_model(\"randomForest\", df, 0.1, 0, n_estimators=n_estimators)\n",
    "        end = time.time()\n",
    "        train_time += [end-start]\n",
    "        acc, prec, rec, f1_, time_, y_pred = test_model(model, X_test, y_test)\n",
    "        accuracy += [acc]\n",
    "        precision += [prec]\n",
    "        recall += [rec]\n",
    "        f1 += [f1_]\n",
    "        test_time += [time_]\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_mean, cv_std = evaluate_with_cv(RandomForestClassifier(n_estimators=n_estimators, random_state=42), X_full, y_full)\n",
    "        cv_scores.append(cv_mean)\n",
    "        \n",
    "        # Track best model\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_n = n_estimators\n",
    "            best_model_data = (model, X_test, y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix and ROC curve for best n_estimators\n",
    "    if best_model_data:\n",
    "        model, X_test, y_test, y_pred = best_model_data\n",
    "        plot_confusion_matrix(y_test, y_pred, f\"Random Forest (n={best_n})\", output_dir)\n",
    "        plot_roc_curve(model, X_test, y_test, f\"Random Forest (n={best_n})\", output_dir)\n",
    "        plot_learning_curve(RandomForestClassifier(n_estimators=best_n, random_state=42), X_full, y_full, f\"Random Forest (n={best_n})\", output_dir)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n----- RANDOM FOREST RESULTS BY NUMBER OF ESTIMATORS -----\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'N Estimators': n_estimators_values,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Score': cv_scores,\n",
    "        'Training Time': train_time,\n",
    "        'Testing Time': test_time\n",
    "    })\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    best_n_idx = np.argmax(accuracy)\n",
    "    print(f\"\\nBest number of estimators: {n_estimators_values[best_n_idx]}\")\n",
    "    print(f\"Maximum accuracy: {accuracy[best_n_idx]:.4f}\")\n",
    "    print(f\"Precision: {precision[best_n_idx]:.4f}\")\n",
    "    print(f\"Recall: {recall[best_n_idx]:.4f}\")\n",
    "    print(f\"F1 Score: {f1[best_n_idx]:.4f}\")\n",
    "    \n",
    "    make_graph_random_forest(\"Accuracy\", n_estimators_values, accuracy, output_dir)\n",
    "    make_graph_random_forest(\"Precision\", n_estimators_values, precision, output_dir)\n",
    "    make_graph_random_forest(\"Recall\", n_estimators_values, recall, output_dir)\n",
    "    make_graph_random_forest(\"F1 Score\", n_estimators_values, f1, output_dir)\n",
    "    make_graph_random_forest(\"CV Score\", n_estimators_values, cv_scores, output_dir)\n",
    "    make_graph_random_forest(\"Training Time\", n_estimators_values, train_time, output_dir)\n",
    "    make_graph_random_forest(\"Testing Time\", n_estimators_values, test_time, output_dir)\n",
    "\n",
    "def compare_svm(df, output_dir):\n",
    "    \"\"\"Compare SVM performance with different C values.\"\"\"\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    cv_scores = []\n",
    "    c_values = [0.1, 0.5, 1, 5, 10, 100]\n",
    "    \n",
    "    # Prepare data for cross-validation\n",
    "    feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                    'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                    'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                    'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                    'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                    'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                    'fighter2_Age']\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp = pd.get_dummies(df_temp, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "    df_temp['target'] = df_temp['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "    df_temp = df_temp.dropna(subset=['target'])\n",
    "    \n",
    "    stance_cols = [col for col in df_temp.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "    X_full = df_temp[feature_cols + stance_cols].fillna(df_temp[feature_cols + stance_cols].median())\n",
    "    y_full = df_temp['target']\n",
    "    \n",
    "    best_c = 0\n",
    "    best_accuracy = 0\n",
    "    best_model_data = None\n",
    "    \n",
    "    for c in c_values:\n",
    "        start = time.time()\n",
    "        result = create_model(\"svm\", df, 0.1, 0, c_value=c)\n",
    "        if len(result) == 4: \n",
    "            model, X_test, y_test, scaler = result\n",
    "        else:\n",
    "            model, X_test, y_test = result\n",
    "        end = time.time()\n",
    "        train_time += [end-start]\n",
    "        acc, prec, rec, f1_, time_, y_pred = test_model(model, X_test, y_test)\n",
    "        accuracy += [acc]\n",
    "        precision += [prec]\n",
    "        recall += [rec]\n",
    "        f1 += [f1_]\n",
    "        test_time += [time_]\n",
    "        \n",
    "        # Cross-validation with scaling\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_full_scaled = scaler_cv.fit_transform(X_full)\n",
    "        cv_mean, cv_std = evaluate_with_cv(SVC(C=c, kernel='rbf', random_state=42), X_full_scaled, y_full)\n",
    "        cv_scores.append(cv_mean)\n",
    "        \n",
    "        # Track best model\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_c = c\n",
    "            best_model_data = (model, X_test, y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix and ROC curve for best C\n",
    "    if best_model_data:\n",
    "        model, X_test, y_test, y_pred = best_model_data\n",
    "        plot_confusion_matrix(y_test, y_pred, f\"SVM (C={best_c})\", output_dir)\n",
    "        plot_roc_curve(model, X_test, y_test, f\"SVM (C={best_c})\", output_dir)\n",
    "        \n",
    "        # For learning curve, we need to create a pipeline with scaling\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        svm_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(C=best_c, kernel='rbf', random_state=42))\n",
    "        ])\n",
    "        plot_learning_curve(svm_pipeline, X_full, y_full, f\"SVM (C={best_c})\", output_dir)\n",
    "    \n",
    "    print(\"\\n----- SVM RESULTS BY C VALUE -----\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'C Value': c_values,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Score': cv_scores,\n",
    "        'Training Time': train_time,\n",
    "        'Testing Time': test_time\n",
    "    })\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    best_c_idx = np.argmax(accuracy)\n",
    "    print(f\"\\nBest C value: {c_values[best_c_idx]}\")\n",
    "    print(f\"Maximum accuracy: {accuracy[best_c_idx]:.4f}\")\n",
    "    print(f\"Precision: {precision[best_c_idx]:.4f}\")\n",
    "    print(f\"Recall: {recall[best_c_idx]:.4f}\")\n",
    "    print(f\"F1 Score: {f1[best_c_idx]:.4f}\")\n",
    "    \n",
    "    make_graph_svm(\"Accuracy\", c_values, accuracy, output_dir)\n",
    "    make_graph_svm(\"Precision\", c_values, precision, output_dir)\n",
    "    make_graph_svm(\"Recall\", c_values, recall, output_dir)\n",
    "    make_graph_svm(\"F1 Score\", c_values, f1, output_dir)\n",
    "    make_graph_svm(\"CV Score\", c_values, cv_scores, output_dir)\n",
    "    make_graph_svm(\"Training Time\", c_values, train_time, output_dir)\n",
    "    make_graph_svm(\"Testing Time\", c_values, test_time, output_dir)\n",
    "\n",
    "def compare_models(df, n_neighbors, output_dir, n_estimators=100, c_value=1.0):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    cv_scores = []\n",
    "    models = [\"Decision Tree\",\"K-Nearest Neighbors\", \"SVM\", \"Random Forest\"]\n",
    "    model_types = [\"decisionTree\",\"K-nearestNeighbors\", \"svm\", \"randomForest\"]\n",
    "    \n",
    "    # Prepare data for cross-validation\n",
    "    feature_cols = ['fighter1_Weight', 'fighter1_Reach','fighter1_SLpM','fighter1_StrAcc','fighter1_SApM',\n",
    "                    'fighter1_StrDef','fighter1_TDAvg','fighter1_TDAcc','fighter1_TDDef','fighter1_SubAvg',\n",
    "                    'fighter2_Weight','fighter2_Reach','fighter2_SLpM','fighter2_StrAcc',\n",
    "                    'fighter2_SApM','fighter2_StrDef','fighter2_TDAvg','fighter2_TDAcc','fighter2_TDDef',\n",
    "                    'fighter2_SubAvg','fighter1_Wins','fighter1_Losses','fighter1_Draws','fighter2_Wins',\n",
    "                    'fighter2_Losses','fighter2_Draws','fighter1_Height_in','fighter2_Height_in','fighter1_Age',\n",
    "                    'fighter2_Age']\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp = pd.get_dummies(df_temp, columns=['fighter1_Stance', 'fighter2_Stance'])\n",
    "    df_temp['target'] = df_temp['fight_outcome'].apply(lambda x: 1 if x == 'fighter1' else (0 if x == 'fighter2' else np.nan))\n",
    "    df_temp = df_temp.dropna(subset=['target'])\n",
    "    \n",
    "    stance_cols = [col for col in df_temp.columns if col.startswith('fighter1_Stance_') or col.startswith('fighter2_Stance_')]\n",
    "    X_full = df_temp[feature_cols + stance_cols].fillna(df_temp[feature_cols + stance_cols].median())\n",
    "    y_full = df_temp['target']\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        acc_model = []\n",
    "        prec_model = []\n",
    "        recall_model = []\n",
    "        f1_model = []\n",
    "        train_time_model = []\n",
    "        test_time_model = []\n",
    "        \n",
    "        for j in range(10):\n",
    "            start = time.time()\n",
    "            result = create_model(model_type, df, 0.1, n_neighbors, n_estimators=n_estimators, c_value=c_value)\n",
    "            if len(result) == 4:\n",
    "                model, X_test, y_test, scaler = result\n",
    "            else:\n",
    "                model, X_test, y_test = result\n",
    "            end = time.time()\n",
    "            train_time_model += [end-start]\n",
    "            acc, prec, rec, f1_, time_, y_pred = test_model(model, X_test, y_test)\n",
    "            acc_model += [acc]\n",
    "            prec_model += [prec]\n",
    "            recall_model += [rec]\n",
    "            f1_model += [f1_]\n",
    "            test_time_model += [time_]\n",
    "        \n",
    "        accuracy += [sum(acc_model)/len(acc_model)]\n",
    "        precision += [sum(prec_model)/len(prec_model)]\n",
    "        recall += [sum(recall_model)/len(recall_model)]\n",
    "        f1 += [sum(f1_model)/len(f1_model)]\n",
    "        train_time += [sum(train_time_model)/len(train_time_model)]\n",
    "        test_time += [sum(test_time_model)/len(test_time_model)]\n",
    "        \n",
    "        # Cross-validation\n",
    "        if model_type == \"decisionTree\":\n",
    "            cv_model = DecisionTreeClassifier(random_state=42)\n",
    "            cv_mean, cv_std = evaluate_with_cv(cv_model, X_full, y_full)\n",
    "        elif model_type == \"K-nearestNeighbors\":\n",
    "            cv_model = KNeighborsClassifier(n_neighbors)\n",
    "            cv_mean, cv_std = evaluate_with_cv(cv_model, X_full, y_full)\n",
    "        elif model_type == \"randomForest\":\n",
    "            cv_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "            cv_mean, cv_std = evaluate_with_cv(cv_model, X_full, y_full)\n",
    "        elif model_type == \"svm\":\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            cv_model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svm', SVC(C=c_value, kernel='rbf', random_state=42))\n",
    "            ])\n",
    "            cv_mean, cv_std = evaluate_with_cv(cv_model, X_full, y_full)\n",
    "        \n",
    "        cv_scores.append(cv_mean)\n",
    "        \n",
    "        # Generate confusion matrix for each model\n",
    "        plot_confusion_matrix(y_test, y_pred, models[i], output_dir)\n",
    "        \n",
    "        # Generate ROC curve for each model\n",
    "        if len(result) == 4:\n",
    "            plot_roc_curve(model, X_test, y_test, models[i], output_dir)\n",
    "        else:\n",
    "            plot_roc_curve(model, X_test, y_test, models[i], output_dir)\n",
    "    \n",
    "    # Print comparison results\n",
    "    print(\"\\n----- COMPARISON BETWEEN DECISION TREE, KNN, RANDOM FOREST AND SVM -----\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV Score': cv_scores,\n",
    "        'Training Time': train_time,\n",
    "        'Testing Time': test_time\n",
    "    })\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    # Identify and print best model\n",
    "    best_model_idx = np.argmax(accuracy)\n",
    "    print(f\"\\nBest model: {models[best_model_idx]}\")\n",
    "    print(f\"Best model accuracy: {accuracy[best_model_idx]:.4f}\")\n",
    "    print(f\"Best model CV score: {cv_scores[best_model_idx]:.4f}\")\n",
    "\n",
    "    make_graph_models(\"Accuracy\", models, accuracy, output_dir)\n",
    "    make_graph_models(\"Precision\", models, precision, output_dir)\n",
    "    make_graph_models(\"Recall\", models, recall, output_dir)\n",
    "    make_graph_models(\"F1 Score\", models, f1, output_dir)\n",
    "    make_graph_models(\"CV Score\", models, cv_scores, output_dir)\n",
    "    make_graph_models(\"Training Time\", models, train_time, output_dir)\n",
    "    make_graph_models(\"Testing Time\", models, test_time, output_dir)\n",
    "\n",
    "def run_model_comparisons(df, output_dir):\n",
    "    \"\"\"\n",
    "    Run all model comparison analyses.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Dataset containing UFC fights\n",
    "        output_dir (str): Directory to save output graphs\n",
    "    \"\"\"\n",
    "    # Create subdirectories for each model\n",
    "    decision_tree_dir = os.path.join(output_dir, \"decision_tree\")\n",
    "    knn_dir = os.path.join(output_dir, \"knn\")\n",
    "    random_forest_dir = os.path.join(output_dir, \"random_forest\")\n",
    "    svm_dir = os.path.join(output_dir, \"svm\")\n",
    "    comparison_dir = os.path.join(output_dir, \"model_comparison\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for directory in [decision_tree_dir, knn_dir, random_forest_dir, svm_dir, comparison_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n===== COMPARING DECISION TREE MODELS =====\")\n",
    "    compare_decision_tree(df, decision_tree_dir)\n",
    "    \n",
    "    print(\"\\n===== COMPARING K-NEAREST NEIGHBORS MODELS =====\")\n",
    "    compare_k_nearest_neighbors(df, knn_dir)\n",
    "    \n",
    "    print(\"\\n===== COMPARING RANDOM FOREST MODELS =====\")\n",
    "    compare_random_forest(df, random_forest_dir)\n",
    "    \n",
    "    print(\"\\n===== COMPARING SVM MODELS =====\")\n",
    "    compare_svm(df, svm_dir)\n",
    "    \n",
    "    print(\"\\n===== COMPARING DIFFERENT MODEL TYPES =====\")\n",
    "    # Use the best parameters found\n",
    "    compare_models(df, 5, comparison_dir, n_estimators=100, c_value=10)\n",
    "    \n",
    "    print(\"\\nModel comparison complete. Graphs saved in subdirectories:\")\n",
    "    print(f\"- Decision Tree: {decision_tree_dir}\")\n",
    "    print(f\"- KNN: {knn_dir}\")\n",
    "    print(f\"- Random Forest: {random_forest_dir}\")\n",
    "    print(f\"- SVM: {svm_dir}\")\n",
    "    print(f\"- Model Comparison: {comparison_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303a9b4-75d2-49e0-9c37-61328d06aba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
